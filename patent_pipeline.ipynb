{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This pipeline is used to scrape protein sequences from a list of patents. It then processes them  to remove short and duplicated sequences, and runs multiple sequences alignments to assess similarity. Any above a specific sequence ID threshold are removed. Filtered sequences are then run through another round of MSA, and phylogenetic trees are generated. Outputs are provided as csv and fasta files.\n",
    "\n",
    "It is currently designed to be run in Google Co-lab, and will not run as a standalone python script. I will fix this at somepoint...\n",
    "\n",
    "The below code functions as a master script, but the individual models are available as stand-alone code if required.\n",
    "\n",
    "Module 1 - Importing libraries and connecting google drive\n",
    "\n",
    "Module 2 - Downloading patent databases\n",
    "This will prompt the user as to whether or not to download the ebi patent databases from their ftp server, and save them to your google drive. You only need to do this once.\n",
    "\n",
    "Module 3 - Patent Sequence Seeker\n",
    "The script inputs a list of patent numbers once stripped of the preceding letters e.g. US, trailing version id (e.g. A1), and all and any punctuation (e.g. 2014/384930), using the cvs_patent_list_reformatter script. It will then search the EU, US, japanese and korean patent databases, grab any sequences from the patent numbers submitted, and save them to your google drive.\n",
    "\n",
    "Module 4 - Faster combiner\n",
    "This will take the individual patent fasta files produced from the above, and combine them into a single file.\n",
    "\n",
    "Module 5 - Patent filter\n",
    "This will parse the combined file from above, and remove any sequences below a specified length, eg 200 residues. It will also remove any duplicates, and has an input to remove specific sequence identifiers or patents not required. It will print an accurate number for the sequences originally loaded, and those in the final file, but the numbers it gives for the ones removed in each class are unreliable (again, I will fix this at some point).\n",
    "\n",
    "Module 6 - Similarity filter\n",
    "This runs a multiple sequence alignment of the filtered sequences from above, and runs a cut off at a specified similarity - the default is set to 95%. It then takes creates a new fasta file with the sequences which are less that 95% similar to each other, and performs the MSA again to confirm they all fall below the cutoff relative to each other.\n",
    "\n",
    "Module 7 - Tree builder\n",
    "This runs another MSA on the filtered sequences and builds a phylogenetic tree. It then has the option to split the tree into separate files at a specified number of branch points. For example, entering ‘2’ will split the original fasta file into two new ones at the first branch point (node1_branch1 and node1_branch2), then split each of these in turn to produce node1_branch1_node2_branch1 etc.\n",
    "\n",
    "Module 8 - blastp (SLOW!)\n",
    "This will take a fasta file input and run blastp. It will then print the top five hits for each sequence ID, and also save them to a csv, including the ncbi URLs of the hits. This works, but is EXTREMELY slow, ie it takes about 15 minutes per sequence, so not recommended unless you really need to. It does have a function to resume from where it left of if unexpected disconnected though.\n",
    "\n",
    "Module 9 - NCBI scrapper (beta)\n",
    "This module takes a list of NCBI links generated by the blastp scrip above, and searches them for particular words, in this case alpha, beta, and gamma. It is very much in beta and only picks up some instances so use with caution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 35612,
     "status": "ok",
     "timestamp": 1718110098448,
     "user": {
      "displayName": "Henry Taunt",
      "userId": "07657040803472197154"
     },
     "user_tz": -60
    },
    "id": "d3WHWB3JWZsi",
    "outputId": "498b55e4-53cd-4017-8217-7963483a467e"
   },
   "outputs": [],
   "source": [
    "# Module 1: Importing libraries\n",
    "%pip install biopython\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "import os\n",
    "from glob import glob\n",
    "import shutil\n",
    "from Bio import SeqIO\n",
    "from Bio.Align.Applications import ClustalOmegaCommandline\n",
    "from Bio.SeqRecord import SeqRecord\n",
    "from Bio.Seq import Seq\n",
    "from Bio import pairwise2\n",
    "from Bio.pairwise2 import format_alignment\n",
    "import csv\n",
    "!apt-get install -y clustalo\n",
    "import subprocess\n",
    "from Bio import Phylo, SeqIO\n",
    "import matplotlib.pyplot as plt\n",
    "from google.colab import files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5418,
     "status": "ok",
     "timestamp": 1717667113327,
     "user": {
      "displayName": "Henry Taunt",
      "userId": "07657040803472197154"
     },
     "user_tz": -60
    },
    "id": "b_EVqfsm9luo",
    "outputId": "0019f3b3-18dc-451b-8bdc-770f34d59941"
   },
   "outputs": [],
   "source": [
    "# Module 2: Download patent databases\n",
    "# Downloading and extracting databases takes a while, but you only need to do it once!\n",
    "\n",
    "# Define the Google Drive folder to save the downloaded files\n",
    "google_drive_folder = '/content/drive/MyDrive/patent_databases/'\n",
    "if not os.path.exists(google_drive_folder):\n",
    "    os.makedirs(google_drive_folder)\n",
    "\n",
    "# Ask the user whether to download the patent databases\n",
    "download_prompt = input(\"Do you want to download the ebi patent databases? (y/n): \").strip().lower()\n",
    "\n",
    "if download_prompt == 'y':\n",
    "    print(\"Downloading and extracting ebi patent databases...\")\n",
    "    # Download files using wget\n",
    "    !wget -P {google_drive_folder} ftp://ftp.ebi.ac.uk/pub/databases/fastafiles/patent/*\n",
    "\n",
    "    # Extract the downloaded files\n",
    "    !gunzip {google_drive_folder}/*.gz\n",
    "else:\n",
    "    print(\"Skipping download and extraction of patent databases.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 938326,
     "status": "ok",
     "timestamp": 1717668051648,
     "user": {
      "displayName": "Henry Taunt",
      "userId": "07657040803472197154"
     },
     "user_tz": -60
    },
    "id": "TW_TBbiRxZCw",
    "outputId": "4d614710-f1bb-41ef-9a9e-894f1db97888"
   },
   "outputs": [],
   "source": [
    "# Module 3: Patent sequence seeker\n",
    "\n",
    "def read_patent_numbers():\n",
    "    patent_numbers = input(\"Enter patent numbers separated by spaces: \").split()\n",
    "    return patent_numbers\n",
    "\n",
    "# Clean up patent numbers to remove non-numerical characters\n",
    "#def read_and_clean_patent_numbers():\n",
    "#    patent_numbers = input(\"Enter patent numbers separated by spaces: \").split()\n",
    "#    cleaned_patent_numbers = [re.sub(r'\\D', '', patent) for patent in patent_numbers]\n",
    "#    return cleaned_patent_numbers\n",
    "\n",
    "\n",
    "def extract_sequences(file_path, patent_number):\n",
    "    command = f\"grep -A 100 {patent_number} {file_path}\"\n",
    "    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "    if result.returncode == 0:\n",
    "        lines = result.stdout.split('\\n')\n",
    "        sequence_lines = []\n",
    "        sequence_started = False\n",
    "        for line in lines:\n",
    "            if sequence_started:\n",
    "                sequence_lines.append(line)\n",
    "            if patent_number in line and not sequence_started:\n",
    "                sequence_started = True\n",
    "                sequence_lines.append(line)\n",
    "        return '\\n'.join(sequence_lines)\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def preprocess_fasta(input_file, output_file):\n",
    "    with open(input_file, \"r\") as input_handle, open(output_file, \"w\") as output_handle:\n",
    "        for line in input_handle:\n",
    "            #output_handle.write(line.replace(\" \", \"_\"))\n",
    "            #output_handle.write(line.replace(\":\", \"_\"))\n",
    "            modified_line = line.replace(\" \", \"_\").replace(\":\", \"_\")\n",
    "            output_handle.write(modified_line)\n",
    "\n",
    "def clean_fasta(input_file, output_file):\n",
    "    filename = os.path.splitext(os.path.basename(input_file))[0]\n",
    "    first_part_filename = filename.split('_')[0]\n",
    "    first_part_filename_no_space = first_part_filename.replace(\" \", \"\")\n",
    "    records_to_keep = {}\n",
    "\n",
    "    print(f\"Search criteria: {first_part_filename_no_space}\")\n",
    "\n",
    "    with open(input_file, \"r\") as handle:\n",
    "        for record in SeqIO.parse(handle, \"fasta\"):\n",
    "            record_id_no_space = record.id.replace(\" \", \"\")\n",
    "            print(f\"Processing sequence: {record.id}\")\n",
    "            if first_part_filename_no_space in record_id_no_space:\n",
    "                seq_str = str(record.seq)\n",
    "                if seq_str not in records_to_keep:\n",
    "                    records_to_keep[seq_str] = record\n",
    "            else:\n",
    "                print(f\"Sequence {record.id} does not contain the search criteria and will be skipped.\")\n",
    "\n",
    "    first_part_filename_file = '_'.join(filename.split('_')[0:2])\n",
    "    clean_folder = os.path.join(google_drive_folder, 'clean_files')\n",
    "    if not os.path.exists(clean_folder):\n",
    "        os.makedirs(clean_folder) # (FILE, exist_ok=true)\n",
    "    output_file = os.path.join(clean_folder, f\"{first_part_filename_file}_clean.fasta\")\n",
    "    with open(output_file, \"w\") as output_handle:\n",
    "        SeqIO.write(records_to_keep.values(), output_handle, \"fasta\")\n",
    "\n",
    "    print(f\"Cleaned sequences saved to {output_file}\")\n",
    "\n",
    "\n",
    "# Function to move files to a 'working files' folder\n",
    "def move_to_working_folder(files):\n",
    "    working_folder = os.path.join(google_drive_folder, 'working_files')\n",
    "    if not os.path.exists(working_folder):\n",
    "        os.makedirs(working_folder)\n",
    "    for file in files:\n",
    "        shutil.move(file, os.path.join(working_folder, os.path.basename(file)))\n",
    "\n",
    "# New function to count patents found in each database\n",
    "def count_patent_occurrences(patent_numbers, databases):\n",
    "  \"\"\"\n",
    "  Counts the number of patents found in each database.\n",
    "\n",
    "  Args:\n",
    "      patent_numbers: A list of cleaned patent numbers.\n",
    "      databases: A list of database file paths.\n",
    "\n",
    "  Returns:\n",
    "      A dictionary where keys are database names and values are counts of patents found.\n",
    "  \"\"\"\n",
    "  patent_counts = {db: 0 for db in databases}\n",
    "  for patent in patent_numbers:\n",
    "    for database in databases:\n",
    "      if extract_sequences(database, patent):\n",
    "        patent_counts[database] += 1\n",
    "  return patent_counts\n",
    "\n",
    "  # Call the new function to count occurrences\n",
    "  patent_occurrences = count_patent_occurrences(patent_numbers, databases)\n",
    "\n",
    "  # Print search results for each database\n",
    "  for database, count in patent_occurrences.items():\n",
    "    print(f\"\\nSearching in {database}...\")\n",
    "    if count > 0:\n",
    "      print(f\"{count} patent(s) found!\")\n",
    "    else:\n",
    "      print(\"No patents found.\")\n",
    "\n",
    "# Function to move files to a 'clean files' folder\n",
    "#def move_to_clean_folder(files):\n",
    "#    clean_folder = os.path.join(google_drive_folder, 'clean_files')\n",
    "#    if not os.path.exists(clean_folder):\n",
    "#        os.makedirs(clean_folder)\n",
    "#    for file in files:\n",
    "#        shutil.move(file, os.path.join(clean_folder, os.path.basename(file)))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    patent_numbers = read_patent_numbers()\n",
    " #   files = [os.path.join(google_drive_folder, f) for f in [\"epo\", \"epoPrt.dat\", \"jpo\", \"jpoPrt.dat\", \"kipo\", \"kipoPrt.dat\", \"uspto\", \"usptoPrt.dat\"]]\n",
    "    files = [os.path.join(google_drive_folder, f) for f in [\"epo\",  \"jpo\", \"kipo\",  \"uspto\"]]\n",
    "\n",
    "    # Call the new function to count occurrences\n",
    "    #patent_occurrences = count_patent_occurrences(patent_numbers, databases)\n",
    "\n",
    "    # Print search results for each database\n",
    "    #for database, count in patent_occurrences.items():\n",
    "    #  print(f\"\\nSearching in {database}...\")\n",
    "    #  if count > 0:\n",
    "    #    print(f\"{count} patent(s) found!\")\n",
    "    #  else:\n",
    "    #    print(\"No patents found.\")\n",
    "\n",
    "    for file in files:\n",
    "        print(f\"\\nSearching in {file}...\")\n",
    "\n",
    "        for patent in patent_numbers:\n",
    "            sequence = extract_sequences(file, patent)\n",
    "            if sequence:\n",
    "                print(f\"Search and extraction successful for {patent} in {file}\")\n",
    "                fasta_filename = os.path.join(google_drive_folder, f\"{patent}_{os.path.basename(file)}.fasta\")\n",
    "                with open(fasta_filename, \"w\") as f:\n",
    "                    f.write(f\">{patent}\\n{sequence}\\n\")  # Ensure proper FASTA format\n",
    "\n",
    "                preprocessed_file = f\"{patent}_{os.path.basename(file)}_preprocessed.fasta\"\n",
    "                preprocess_fasta(fasta_filename, preprocessed_file)\n",
    "\n",
    "                clean_fasta_output_file = f\"{patent}_{os.path.basename(file)}_clean.fasta\"\n",
    "                clean_fasta(preprocessed_file, clean_fasta_output_file)\n",
    "\n",
    "                move_to_working_folder([fasta_filename, preprocessed_file])\n",
    "\n",
    "            else:\n",
    "                print(f\"Patent number {patent} not found in {file}\")\n",
    "\n",
    "# Call the new function to count occurrences\n",
    "#patent_occurrences = count_patent_occurrences(patent_numbers, databases)\n",
    "\n",
    "#Print search results for each database\n",
    "#for database, count in patent_occurrences.items():\n",
    "#  print(f\"\\nSearching in {database}...\")\n",
    "#  if count > 0:\n",
    "#    print(f\"{count} patent(s) found!\")\n",
    "#  else:\n",
    "#    print(\"No patents found.\")\n",
    "\n",
    "print(\"It's not finished. It's finished.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1717668052536,
     "user": {
      "displayName": "Henry Taunt",
      "userId": "07657040803472197154"
     },
     "user_tz": -60
    },
    "id": "q0qAhkiCXT2V",
    "outputId": "6311de9b-63e1-453a-a33e-e155532aa2c4"
   },
   "outputs": [],
   "source": [
    "# Module 4: Combine clean fasta files into single entity\n",
    "\n",
    "def combine_fasta_files(input_directory, output_file):\n",
    "    \"\"\"\n",
    "    Combine multiple FASTA files into a single FASTA file.\n",
    "\n",
    "    Parameters:\n",
    "    input_directory (str): Directory containing the input FASTA files.\n",
    "    output_file (str): Path to the output combined FASTA file.\n",
    "    \"\"\"\n",
    "    # Get a list of all FASTA files in the input directory\n",
    "    fasta_files = glob(os.path.join(input_directory, '*.fasta'))\n",
    "\n",
    "    # Open the output file for writing\n",
    "    with open(output_file, 'w') as outfile:\n",
    "        for fasta_file in fasta_files:\n",
    "            with open(fasta_file, 'r') as infile:\n",
    "                # Write the content of the current FASTA file to the output file\n",
    "                for line in infile:\n",
    "                    outfile.write(line)\n",
    "\n",
    "# Define the input directory and output file paths in Google Drive\n",
    "input_directory = \"/content/drive/MyDrive/patent_databases/clean_files/\"  # Change to your input directory\n",
    "output_file = \"/content/drive/My Drive/patent_databases/clean_files/combined.fasta\"  # Change to your desired output file path\n",
    "\n",
    "# Combine FASTA files\n",
    "combine_fasta_files(input_directory, output_file)\n",
    "print(f\"Combined FASTA file written to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5JBhX_Wgl38"
   },
   "outputs": [],
   "source": [
    "# Module 5a - Filter sequences by legnth, duplicates, and specific text inputs - definitions\n",
    "# Define subroutines\n",
    "\n",
    "min_length= int( input (\"enter sequence length cutoff: \"))\n",
    "\n",
    "def parse_fasta(output_file):\n",
    "    sequences = {}\n",
    "    with open(input_file, 'r') as f:\n",
    "        current_id = None\n",
    "        current_seq = ''\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith('>'):\n",
    "                if current_id is not None:\n",
    "                    sequences[current_id] = current_seq\n",
    "                current_id = line[1:]\n",
    "                current_seq = ''\n",
    "            else:\n",
    "                current_seq += line\n",
    "        if current_id is not None:\n",
    "            sequences[current_id] = current_seq\n",
    "    return sequences\n",
    "\n",
    "# Run filters: Length, custom text, duplicatets\n",
    "\n",
    "def filter_sequences(sequences, min_length=min_length, remove_texts=None):\n",
    "    original_count = len(sequences)\n",
    "    filtered_sequences = {}\n",
    "    removed_duplicates = []\n",
    "    removed_length = []\n",
    "    unique_patent_sequence_names = set()\n",
    "    for seq_id, seq in sequences.items():\n",
    "        if remove_texts and any(text in seq_id for text in remove_texts):\n",
    "            continue  # Skip sequences containing any of the specified texts\n",
    "        if len(seq) >= min_length:\n",
    "            is_duplicate = False\n",
    "            for other_id, other_seq in sequences.items():\n",
    "                if seq_id != other_id and seq in other_seq:\n",
    "                    is_duplicate = True\n",
    "                    removed_duplicates.append(seq_id)\n",
    "                    break\n",
    "            if not is_duplicate:\n",
    "                filtered_sequences[seq_id] = seq\n",
    "                if \"patent\" in seq_id:\n",
    "                    _, seq_name = seq_id.split(\"patent\", 1)\n",
    "                    unique_patent_sequence_names.add(seq_name.strip())\n",
    "                if \"Patent\" in seq_id:\n",
    "                    _, seq_name = seq_id.split(\"Patent\", 1)\n",
    "                    unique_patent_sequence_names.add(seq_name.strip())\n",
    "            else:\n",
    "                removed_length.append(seq_id)\n",
    "    remaining_count = len(filtered_sequences)\n",
    "    removed_duplicates_count = len(removed_duplicates)\n",
    "    removed_length_count = len(removed_length)\n",
    "    return filtered_sequences, removed_duplicates_count, removed_length_count, remaining_count, original_count, unique_patent_sequence_names\n",
    "\n",
    "# Write output fasta and csv files\n",
    "\n",
    "def write_fasta(output_file, sequences):\n",
    "    with open(output_file, 'w') as f:\n",
    "        for seq_id, seq in sequences.items():\n",
    "            f.write(f'>{seq_id}\\n')\n",
    "            f.write(f'{seq}\\n')\n",
    "\n",
    "def write_csv(output_file, sequences):\n",
    "    with open(output_file, 'w', newline='') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        writer.writerow([\"Sequence ID\", \"Sequence\"])\n",
    "        for seq_id, seq in sequences.items():\n",
    "            writer.writerow([seq_id, seq])\n",
    "\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# Assuming parse_fasta and filter_sequences functions are already defined elsewhere\n",
    "\n",
    "def remove_duplicate_strings(header):\n",
    "    parts = header.split('_')\n",
    "    seen = set()\n",
    "    new_parts = []\n",
    "    for part in parts:\n",
    "        if part not in seen:\n",
    "            seen.add(part)\n",
    "            new_parts.append(part)\n",
    "    return '_'.join(new_parts)\n",
    "\n",
    "def filter_and_save(input_file, remove_texts=None):\n",
    "    sequences = parse_fasta(input_file)\n",
    "\n",
    "    # Plot histogram before filtering\n",
    "    plot_histogram(sequences, 'Histogram of Sequence Lengths Before Filtering')\n",
    "\n",
    "    filtered_sequences, removed_duplicates_count, removed_length_count, remaining_count, original_count, unique_patent_sequence_names = filter_sequences(sequences, remove_texts=remove_texts)\n",
    "\n",
    "    # Plot histogram after filtering\n",
    "    plot_histogram(filtered_sequences, 'Histogram of Sequence Lengths After Filtering')\n",
    "\n",
    "    output_file_base = os.path.splitext(input_file)[0]\n",
    "    fasta_output_file = f\"{output_file_base}_filtered.fasta\"\n",
    "    csv_output_file = f\"{output_file_base}_filtered.csv\"\n",
    "\n",
    "    # Replace colons in sequence IDs with underscores and remove duplicate strings\n",
    "    filtered_sequences = {remove_duplicate_strings(seq_id.replace(':', '_')): seq for seq_id, seq in filtered_sequences.items()}\n",
    "\n",
    "    write_fasta(fasta_output_file, filtered_sequences)\n",
    "    write_csv(csv_output_file, filtered_sequences)\n",
    "\n",
    "    return original_count, removed_duplicates_count, removed_length_count, remaining_count, unique_patent_sequence_names\n",
    "\n",
    "\n",
    "def plot_histogram(sequences, title):\n",
    "    lengths = [len(seq) for seq in sequences.values()]\n",
    "    plt.hist(lengths, bins=range(0, max(lengths) + 20, 20), edgecolor='black')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Sequence Length')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 15693,
     "status": "ok",
     "timestamp": 1717687949791,
     "user": {
      "displayName": "Henry Taunt",
      "userId": "07657040803472197154"
     },
     "user_tz": -60
    },
    "id": "saDU6xtUMDbF",
    "outputId": "c1a641ea-2195-457a-f45e-74e3048e51d1"
   },
   "outputs": [],
   "source": [
    "# Module 5b - Filter sequences by legnth, duplicates, and specific text inputs - Main code\n",
    "from google.colab import files\n",
    "if __name__ == \"__main__\":\n",
    "    #uploaded = files.upload()\n",
    "    #input_file = list(uploaded.keys())[0]  # Use the first uploaded file\n",
    "    #input_file = output_file\n",
    "    input_file = \"/content/drive/MyDrive/patent_databases/clean_files/combined.fasta\"\n",
    "    #output_file = \"filtered.fasta\"\n",
    "    remove_texts_input = input(\"Enter space-separated list of texts to remove from sequence IDs (leave blank if none): \").split(' ')\n",
    "    #remove_texts_input = \"\"\n",
    "    remove_texts = [text.strip() for text in remove_texts_input if text.strip()]\n",
    "\n",
    "    original_count, removed_duplicates_count, removed_length_count, remaining_count, unique_patent_sequence_names = filter_and_save(input_file, remove_texts)\n",
    "\n",
    "    output_file_base = os.path.splitext(input_file)[0]\n",
    "    fasta_output_file = f\"{output_file_base}_filtered.fasta\"\n",
    "\n",
    "    # Print results and URLs\n",
    "    print(\"Number of original sequences:\", original_count)\n",
    "    print(\"Number removed based on identical sequences:\", removed_duplicates_count)\n",
    "    print(\"Number removed based on length:\", removed_length_count)\n",
    "    print(\"Number remaining:\", remaining_count)\n",
    "    print(\"List of unique patent numbers:\")\n",
    "    for patent_number in unique_patent_sequence_names:\n",
    "        print(patent_number.replace(' ', ''))\n",
    "        patent_url = f\"https://patents.google.com/?q={patent_number.replace(' ', '')}\"\n",
    "        print(f\"Patent: {patent_number} - URL: {patent_url}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "executionInfo": {
     "elapsed": 338,
     "status": "error",
     "timestamp": 1718110052524,
     "user": {
      "displayName": "Henry Taunt",
      "userId": "07657040803472197154"
     },
     "user_tz": -60
    },
    "id": "SYKr1QyL-RYf",
    "outputId": "7bd5f5f1-b963-4989-82fe-a17f43ddc1a9"
   },
   "outputs": [],
   "source": [
    "# Module 6 - Similarity filter with gap remover\n",
    "\n",
    "# Function to filter sequences in a single file\n",
    "def filter_sequences_in_file(input_file, threshold=95):\n",
    "    sequences = [record for record in SeqIO.parse(input_file, \"fasta\") if len(record.seq) > 0]\n",
    "\n",
    "    if not sequences:\n",
    "        raise ValueError(\"No valid sequences found in the input file.\")\n",
    "\n",
    "    # Write sequences to a temporary file for MSA\n",
    "    temp_input_file = \"temp.fasta\"\n",
    "    SeqIO.write(sequences, temp_input_file, \"fasta\")\n",
    "\n",
    "    # Perform Clustal Omega alignment\n",
    "    clustal_output = \"aligned.aln\"\n",
    "    clustalomega_cline = ClustalOmegaCommandline(infile=temp_input_file, outfile=clustal_output, verbose=True, auto=True, force=True)\n",
    "    stdout, stderr = clustalomega_cline()\n",
    "\n",
    "    # Read the aligned sequences\n",
    "    aligned_sequences = list(SeqIO.parse(clustal_output, \"fasta\"))\n",
    "\n",
    "    # Filter sequences based on similarity\n",
    "    filtered_sequences = []\n",
    "    for record in aligned_sequences:\n",
    "        is_unique = True\n",
    "        for filtered_record in filtered_sequences:\n",
    "            similarity = sum(a == b for a, b in zip(record.seq, filtered_record.seq)) / min(len(record.seq), len(filtered_record.seq))\n",
    "            if similarity >= threshold / 100:\n",
    "                is_unique = False\n",
    "                break\n",
    "        if is_unique:\n",
    "            filtered_sequences.append(record)\n",
    "\n",
    "    # Write filtered sequences to a new FASTA file\n",
    "    input_base_name = os.path.splitext(os.path.basename(input_file))[0]\n",
    "    filtered_file = f\"{input_base_name}_filtered_{threshold}.fasta\"\n",
    "    SeqIO.write(filtered_sequences, filtered_file, \"fasta\")\n",
    "    return filtered_file, sequences, aligned_sequences, filtered_sequences\n",
    "\n",
    "# Function to remove gaps from sequences\n",
    "def remove_gaps_from_sequences(input_file, output_file):\n",
    "    sequences = SeqIO.parse(input_file, \"fasta\")\n",
    "    cleaned_sequences = []\n",
    "\n",
    "    for record in sequences:\n",
    "        record.seq = record.seq.replace(\"-\", \"\")\n",
    "        cleaned_sequences.append(record)\n",
    "\n",
    "    with open(output_file, \"w\") as output_handle:\n",
    "        SeqIO.write(cleaned_sequences, output_handle, \"fasta\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Upload a FASTA file\n",
    "    uploaded = files.upload()\n",
    "    input_file = list(uploaded.keys())[0]  # Use the first uploaded file\n",
    "\n",
    "    #for input_file in new_files:\n",
    "    #input_file = fasta_output_file #from main filter above\n",
    "\n",
    "    # Step 2: Apply the similarity filter\n",
    "    filtered_file, original_sequences, aligned_sequences, filtered_sequences = filter_sequences_in_file(input_file)\n",
    "\n",
    "    # Step 3: Remove gaps from the filtered sequences and save to a new FASTA file\n",
    "    output_file = os.path.splitext(filtered_file)[0] + \"_cleaned.fasta\"\n",
    "    remove_gaps_from_sequences(filtered_file, output_file)\n",
    "\n",
    "    # Print original files with similarity percentages\n",
    "    print(f\"Original sequences in {input_file} with similarity % to their closest match:\")\n",
    "    for record in aligned_sequences:\n",
    "        closest_similarity = 0\n",
    "        for other_record in aligned_sequences:\n",
    "            if record.id != other_record.id:\n",
    "                similarity = sum(a == b for a, b in zip(record.seq, other_record.seq)) / min(len(record.seq), len(other_record.seq)) * 100\n",
    "                if similarity > closest_similarity:\n",
    "                    closest_similarity = similarity\n",
    "        print(f\"{record.id}: {closest_similarity:.2f}%\")\n",
    "\n",
    "    # Print filtered sequences with similarity percentages\n",
    "    print(f\"\\nFiltered sequences in {filtered_file} with similarity % to their closest match in the filtered list:\")\n",
    "    for record in filtered_sequences:\n",
    "        closest_similarity = 0\n",
    "        for other_record in filtered_sequences:\n",
    "            if record.id != other_record.id:\n",
    "                similarity = sum(a == b for a, b in zip(record.seq, other_record.seq)) / min(len(record.seq), len(other_record.seq)) * 100\n",
    "                if similarity > closest_similarity:\n",
    "                    closest_similarity = similarity\n",
    "        print(f\"{record.id}: {closest_similarity:.2f}%\")\n",
    "\n",
    "print(f\"\\nNumber of input sequences: {len(original_sequences)}\")\n",
    "print(f\"Number of output sequences: {len(filtered_sequences)}\")\n",
    "print(\"\")\n",
    "print(f\"\\nFiltered and cleaned sequences saved to: {output_file} from input file: {input_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 324809,
     "status": "ok",
     "timestamp": 1718110590317,
     "user": {
      "displayName": "Henry Taunt",
      "userId": "07657040803472197154"
     },
     "user_tz": -60
    },
    "id": "xGEBP1dnWcQu",
    "outputId": "8cf48945-6b31-425b-e4f2-afdce6347cf6"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from Bio import SeqIO\n",
    "from Bio.Align.Applications import ClustalOmegaCommandline\n",
    "from google.colab import files\n",
    "\n",
    "# Function to filter sequences in a single file\n",
    "def filter_sequences_in_file(input_file, threshold=95):\n",
    "    sequences = [record for record in SeqIO.parse(input_file, \"fasta\") if len(record.seq) > 0]\n",
    "\n",
    "    if not sequences:\n",
    "        raise ValueError(\"No valid sequences found in the input file.\")\n",
    "\n",
    "    # Write sequences to a temporary file for MSA\n",
    "    temp_input_file = \"temp.fasta\"\n",
    "    SeqIO.write(sequences, temp_input_file, \"fasta\")\n",
    "\n",
    "    # Perform Clustal Omega alignment\n",
    "    clustal_output = \"aligned.aln\"\n",
    "    clustalomega_cline = ClustalOmegaCommandline(infile=temp_input_file, outfile=clustal_output, verbose=True, auto=True, force=True)\n",
    "    stdout, stderr = clustalomega_cline()\n",
    "\n",
    "    # Read the aligned sequences\n",
    "    aligned_sequences = list(SeqIO.parse(clustal_output, \"fasta\"))\n",
    "\n",
    "    # Filter sequences based on similarity\n",
    "    filtered_sequences = []\n",
    "    removed_sequences = []\n",
    "    for record in aligned_sequences:\n",
    "        is_unique = True\n",
    "        for filtered_record in filtered_sequences:\n",
    "            similarity = sum(a == b for a, b in zip(record.seq, filtered_record.seq)) / min(len(record.seq), len(filtered_record.seq))\n",
    "            if similarity >= threshold / 100:\n",
    "                is_unique = False\n",
    "                removed_sequences.append(record)\n",
    "                break\n",
    "        if is_unique:\n",
    "            filtered_sequences.append(record)\n",
    "\n",
    "    # Write filtered sequences to a new FASTA file\n",
    "    input_base_name = os.path.splitext(os.path.basename(input_file))[0]\n",
    "    filtered_file = f\"{input_base_name}_filtered_{threshold}.fasta\"\n",
    "    SeqIO.write(filtered_sequences, filtered_file, \"fasta\")\n",
    "    return filtered_file, sequences, aligned_sequences, filtered_sequences, removed_sequences\n",
    "\n",
    "# Function to remove gaps from sequences\n",
    "def remove_gaps_from_sequences(input_file, output_file):\n",
    "    sequences = SeqIO.parse(input_file, \"fasta\")\n",
    "    cleaned_sequences = []\n",
    "\n",
    "    for record in sequences:\n",
    "        record.seq = record.seq.replace(\"-\", \"\")\n",
    "        cleaned_sequences.append(record)\n",
    "\n",
    "    with open(output_file, \"w\") as output_handle:\n",
    "        SeqIO.write(cleaned_sequences, output_handle, \"fasta\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Step 1: Upload a FASTA file\n",
    "    uploaded = files.upload()\n",
    "    input_file = list(uploaded.keys())[0]  # Use the first uploaded file\n",
    "\n",
    "    # Step 2: Apply the similarity filter\n",
    "    filtered_file, original_sequences, aligned_sequences, filtered_sequences, removed_sequences = filter_sequences_in_file(input_file)\n",
    "\n",
    "    # Step 3: Remove gaps from the filtered sequences and save to a new FASTA file\n",
    "    output_file = os.path.splitext(filtered_file)[0] + \"_cleaned.fasta\"\n",
    "    remove_gaps_from_sequences(filtered_file, output_file)\n",
    "\n",
    "    # Calculate and print similarity percentages\n",
    "    def calculate_similarity_percentages(sequences):\n",
    "        similarities = []\n",
    "        for i, record in enumerate(sequences):\n",
    "            closest_similarity = 0\n",
    "            for j, other_record in enumerate(sequences):\n",
    "                if i != j:\n",
    "                    similarity = sum(a == b for a, b in zip(record.seq, other_record.seq)) / min(len(record.seq), len(other_record.seq)) * 100\n",
    "                    if similarity > closest_similarity:\n",
    "                        closest_similarity = similarity\n",
    "            similarities.append(closest_similarity)\n",
    "        return similarities\n",
    "\n",
    "    original_similarities = calculate_similarity_percentages(aligned_sequences)\n",
    "    filtered_similarities = calculate_similarity_percentages(filtered_sequences)\n",
    "\n",
    "    # Print original sequences with similarity percentages\n",
    "    print(f\"Original sequences in {input_file} with similarity % to their closest match:\")\n",
    "    for record, similarity in zip(aligned_sequences, original_similarities):\n",
    "        print(f\"{record.id}: {similarity:.2f}%\")\n",
    "\n",
    "    # Print filtered sequences with similarity percentages\n",
    "    print(f\"\\nFiltered sequences in {filtered_file} with similarity % to their closest match in the filtered list:\")\n",
    "    for record, similarity in zip(filtered_sequences, filtered_similarities):\n",
    "        print(f\"{record.id}: {similarity:.2f}%\")\n",
    "\n",
    "    # Print removed sequences\n",
    "    print(f\"\\nRemoved sequences from the filtered list:\")\n",
    "    for record in removed_sequences:\n",
    "        print(f\"{record.id}\")\n",
    "\n",
    "    # Plot histograms of similarity percentages\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(original_similarities, bins=20, color='blue', alpha=0.7)\n",
    "    plt.title('Original Sequences Similarity')\n",
    "    plt.xlabel('Similarity %')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.hist(filtered_similarities, bins=20, color='green', alpha=0.7)\n",
    "    plt.title('Filtered Sequences Similarity')\n",
    "    plt.xlabel('Similarity %')\n",
    "    plt.ylabel('Frequency')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"\\nNumber of input sequences: {len(original_sequences)}\")\n",
    "    print(f\"Number of output sequences: {len(filtered_sequences)}\")\n",
    "    print(\"\")\n",
    "    print(f\"\\nFiltered and cleaned sequences saved to: {output_file} from input file: {input_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "output_embedded_package_id": "1L9LbsBMN1_jF_TT6vcU3nSQ_x5zyn7Dq"
    },
    "executionInfo": {
     "elapsed": 130111,
     "status": "ok",
     "timestamp": 1718111916232,
     "user": {
      "displayName": "Henry Taunt",
      "userId": "07657040803472197154"
     },
     "user_tz": -60
    },
    "id": "uleh3NyDqHtA",
    "outputId": "fb151458-2b7c-4b5a-c028-3334d8ec6239"
   },
   "outputs": [],
   "source": [
    "# Modeule 7 - Current - with colours, still a bit buggy\n",
    "\n",
    "# Function to extract and split sequence identifiers\n",
    "#def extract_sequence_identifiers(tree_file):\n",
    "#    tree = Phylo.read(tree_file, \"newick\")\n",
    "#    modified_names = {}\n",
    "#    for leaf in tree.get_terminals():\n",
    "#        parts = leaf.name.split('_')\n",
    "#        if len(parts) > 2:\n",
    "#            new_name = f\"{parts[1]}_{parts[-1]}\"\n",
    "#            modified_names[leaf.name] = new_name\n",
    "#            leaf.name = new_name\n",
    "#        else:\n",
    "#            modified_names[leaf.name] = leaf.name\n",
    "#    return modified_names\n",
    "\n",
    "\n",
    "# Function to extract sequence identifiers\n",
    "def extract_sequence_identifiers(tree_file):\n",
    "    tree = Phylo.read(tree_file, \"newick\")\n",
    "    return [leaf.name for leaf in tree.get_terminals()]\n",
    "\n",
    "# Function to parse the Newick tree to obtain the first branch point\n",
    "def extract_first_branch_point(tree):\n",
    "    for node in tree.find_clades(order='level'):\n",
    "        if not node.is_terminal() and len(node.clades) == 2:  # Assuming binary tree\n",
    "            return node\n",
    "    return None\n",
    "\n",
    "# Function to split the tree into clusters at the first branch point\n",
    "def split_tree_into_clusters(tree, branch_point):\n",
    "    clusters = {}\n",
    "    for i, child in enumerate(branch_point.clades):\n",
    "        cluster_name = f\"branch{i + 1}\"\n",
    "        clusters[cluster_name] = [leaf.name for leaf in child.get_terminals()]\n",
    "    return clusters\n",
    "\n",
    "# Function to write sequences of each cluster into new FASTA files\n",
    "def write_fasta_file(cluster, cluster_name, original_sequences):\n",
    "    with open(f\"{cluster_name}.fasta\", 'w') as f:\n",
    "        for seq_id in cluster:\n",
    "            seq_record = original_sequences[seq_id]\n",
    "            SeqIO.write(seq_record, f, \"fasta\")\n",
    "\n",
    "# Function to convert matplotlib color to hex\n",
    "def rgb_to_hex(color):\n",
    "    return \"#{:02x}{:02x}{:02x}\".format(int(color[0]*255), int(color[1]*255), int(color[2]*255))\n",
    "\n",
    "# Function to draw the tree with color-coded clusters\n",
    "def draw_tree(tree_file, num_sequences, clusters=None):\n",
    "    fig_width = 25  # Increased width to fit all text\n",
    "    fig_height = max(10, num_sequences * 0.15)  # Adjust the height dynamically\n",
    "    fig = plt.figure(figsize=(fig_width, fig_height))  # Adjust the figure size to ensure identifiers do not overlap\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    tree = Phylo.read(tree_file, \"newick\")\n",
    "\n",
    "#    # Modify sequence IDs before drawing\n",
    "#    original_to_new_names = {}\n",
    "#    for leaf in tree.get_terminals():\n",
    "#        parts = leaf.name.split('_')\n",
    "#        if len(parts) > 2:\n",
    "#            new_name = f\"{parts[1]}_{parts[-1]}\"\n",
    "#            original_to_new_names[leaf.name] = new_name\n",
    "#            leaf.name = new_name\n",
    "#        else:\n",
    "#            original_to_new_names[leaf.name] = leaf.name\n",
    "\n",
    "    if clusters:\n",
    "        # Update clusters with new sequence IDs\n",
    "        updated_clusters = {}\n",
    "        for cluster_name, seq_ids in clusters.items():\n",
    "            updated_seq_ids = [original_to_new_names[seq_id] for seq_id in seq_ids]\n",
    "            updated_clusters[cluster_name] = updated_seq_ids\n",
    "\n",
    "        # Color code clusters\n",
    "        colors = plt.get_cmap('tab10')\n",
    "        for i, (cluster_name, seq_ids) in enumerate(updated_clusters.items()):\n",
    "            color = rgb_to_hex(colors(i % 10))\n",
    "            for leaf in tree.get_terminals():\n",
    "                if leaf.name in seq_ids:\n",
    "                    leaf.color = color\n",
    "\n",
    "    Phylo.draw(tree, do_show=False, axes=ax)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Step 1: Upload a multipart FASTA protein file\n",
    "uploaded = files.upload()\n",
    "uploaded_files = list(uploaded.keys())\n",
    "\n",
    "#uploaded = output_file # from similarity filter above\n",
    "\n",
    "# Convert uploaded files to list\n",
    "#uploaded_files = list(uploaded)\n",
    "\n",
    "\n",
    "\n",
    "# Function to run Clustal Omega for alignment and tree generation\n",
    "def run_clustalo(fasta_file, iteration):\n",
    "    iteration_str = f\"_iter{iteration}\" if iteration >= 1 else \"\"  # Append iteration number only if it's not the first iteration\n",
    "    alignment_file = f\"{os.path.splitext(fasta_file)[0]}{iteration_str}.aln\"\n",
    "    tree_file = f\"{os.path.splitext(fasta_file)[0]}{iteration_str}.dnd\"\n",
    "    subprocess.run([\"clustalo\", \"-i\", fasta_file, \"-o\", alignment_file])\n",
    "    subprocess.run([\"clustalo\", \"--guidetree-out\", tree_file, \"-i\", alignment_file])\n",
    "    print(f\"Iteration {iteration}: Alignment file - {alignment_file}, Tree file - {tree_file}\")  # Add this line for debugging\n",
    "    print(\"\")  # Add this line for debugging\n",
    "\n",
    "    return alignment_file, tree_file\n",
    "\n",
    "\n",
    "\n",
    "# Prompt the user for the number of iterations\n",
    "num_iterations = int(input(\"Enter the number of branch points to split the tree: \"))\n",
    "\n",
    "# Process each uploaded file\n",
    "for file_name in uploaded_files:\n",
    "    original_sequences = SeqIO.to_dict(SeqIO.parse(file_name, \"fasta\"))\n",
    "    current_files = [file_name]\n",
    "    clusters_overall = {}\n",
    "    original_to_new_names = {}  # Define the dictionary here\n",
    "\n",
    "    # Iterate through each iteration\n",
    "    for iteration in range(num_iterations):\n",
    "        new_files = []\n",
    "\n",
    "        # Iterate through files for the current iteration\n",
    "        for current_file_iter in current_files:\n",
    "            # Step 2: Run sequences through Clustal Omega for multiple sequence alignment\n",
    "            alignment_file, tree_file = run_clustalo(current_file_iter, iteration)\n",
    "\n",
    "            # Step 3: Check if the tree file is created\n",
    "            if not os.path.isfile(tree_file):\n",
    "                print(f\"Tree file {tree_file} not found. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            # Step 4: Build and draw a Newick tree from the alignment\n",
    "            tree = Phylo.read(tree_file, \"newick\")\n",
    "            if iteration == 0:\n",
    "                draw_tree(tree_file, len(tree.get_terminals()))\n",
    "\n",
    "            # Step 5: Extract the first branch point\n",
    "            branch_point = extract_first_branch_point(tree)\n",
    "            if not branch_point:\n",
    "                print(f\"No valid branch point found in iteration {iteration + 1} for file {current_file_iter}\")\n",
    "                continue\n",
    "\n",
    "            # Step 6: Split the tree into clusters at the branch point\n",
    "            clusters = split_tree_into_clusters(tree, branch_point)\n",
    "\n",
    "            # Update the overall clusters dictionary with new sequence IDs\n",
    "            for cluster_name, cluster in clusters.items():\n",
    "                # Filter out sequence IDs that do not have a corresponding entry in original_to_new_names\n",
    "                valid_seq_ids = [seq_id for seq_id in cluster if seq_id in original_to_new_names]\n",
    "                clusters_overall[f\"{os.path.splitext(current_file_iter)[0]}_node{iteration+1}_{cluster_name}\"] = [\n",
    "                    original_to_new_names[seq_id] for seq_id in valid_seq_ids\n",
    "                ]\n",
    "\n",
    "\n",
    "            # Step 7: Write sequences of each cluster into new FASTA files\n",
    "            for cluster_name, cluster in clusters.items():\n",
    "                new_fasta_file = f\"{os.path.splitext(current_file_iter)[0]}_node{iteration+1}_{cluster_name}\"\n",
    "                write_fasta_file(cluster, new_fasta_file, original_sequences)\n",
    "                new_files.append(new_fasta_file + \".fasta\")\n",
    "\n",
    "        # Update the list of files for the next iteration\n",
    "        current_files = new_files\n",
    "\n",
    "        # Draw the new trees for each new file\n",
    "        for new_file in new_files:\n",
    "            alignment_file, tree_file = run_clustalo(new_file, iteration + 1)\n",
    "            if os.path.isfile(tree_file):\n",
    "                draw_tree(tree_file, len(extract_sequence_identifiers(tree_file)))\n",
    "\n",
    "    # Draw the final tree with clusters color-coded\n",
    "    original_tree_file = run_clustalo(file_name, 0)[1]\n",
    "    if os.path.isfile(original_tree_file):\n",
    "        draw_tree(original_tree_file, len(extract_sequence_identifiers(original_tree_file)), clusters_overall)\n",
    "\n",
    "    print(f\"Processed file: {file_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 18767992,
     "status": "ok",
     "timestamp": 1717624261123,
     "user": {
      "displayName": "Henry Taunt",
      "userId": "07657040803472197154"
     },
     "user_tz": -60
    },
    "id": "tfldXq4dQ1XW",
    "outputId": "eed70ecd-6bb8-44d0-b50b-734a6cd21419"
   },
   "outputs": [],
   "source": [
    "# Module 8 - Blastp SLOW!!\n",
    "\n",
    "from Bio.Blast import NCBIWWW, NCBIXML\n",
    "import csv\n",
    "import requests\n",
    "from time import sleep\n",
    "from xml.etree import ElementTree as ET\n",
    "\n",
    "uploaded = files.upload()\n",
    "\n",
    "fasta_file = list(uploaded.keys())[0]\n",
    "\n",
    "# Function to perform BLASTP search and store top 5 hits\n",
    "def blastp_and_store_hits(seq_record, csv_writer):\n",
    "    query_id = seq_record.id\n",
    "    print(\"Query ID:\", query_id)\n",
    "    result_handle = NCBIWWW.qblast(\"blastp\", \"nr\", seq_record.seq)\n",
    "    blast_records = NCBIXML.parse(result_handle)\n",
    "    for i, blast_record in enumerate(blast_records):\n",
    "        if i >= 5:  # Limit to top 5 hits\n",
    "            break\n",
    "        for alignment in blast_record.alignments[:5]:\n",
    "            hit_id = alignment.hit_id.split(\"|\")[1]  # Extract NCBI ID\n",
    "            hit_description = aliågnment.title\n",
    "            hit_link = f\"https://www.ncbi.nlm.nih.gov/protein/{hit_id}\"\n",
    "            csv_writer.writerow([query_id, hit_description, hit_link])\n",
    "            print(\"   Hit:\", hit_description)\n",
    "    result_handle.close()\n",
    "\n",
    "# Function to read the last processed query ID from the CSV file\n",
    "def get_last_processed_query_id():\n",
    "    try:\n",
    "        with open(\"blast_results.csv\", \"r\", newline=\"\") as csv_file:\n",
    "            csv_reader = csv.reader(csv_file)\n",
    "            last_row = None\n",
    "            for row in csv_reader:\n",
    "                last_row = row\n",
    "            if last_row:\n",
    "                return last_row[0]  # Return the query ID from the last row\n",
    "    except FileNotFoundError:\n",
    "        pass  # If the file doesn't exist yet, return None\n",
    "    return None\n",
    "\n",
    "# Upload protein FASTA file\n",
    "from google.colab import files\n",
    "uploaded = files.upload()\n",
    "\n",
    "# Open a CSV file for writing or appending\n",
    "with open(\"blast_results.csv\", \"a\", newline=\"\") as csv_file:\n",
    "    csv_writer = csv.writer(csv_file)\n",
    "\n",
    "    # If the file is empty, write the header row\n",
    "    if csv_file.tell() == 0:\n",
    "        csv_writer.writerow([\"Query ID\", \"Hit Description\", \"NCBI Link\"])\n",
    "\n",
    "    # Get the last processed query ID\n",
    "    last_processed_query_id = get_last_processed_query_id()\n",
    "\n",
    "    # Iterate over each sequence in the uploaded FASTA file\n",
    "    for filename in uploaded.keys():\n",
    "        with open(filename, \"r\") as fasta_file:\n",
    "            for seq_record in SeqIO.parse(fasta_file, \"fasta\"):\n",
    "                if last_processed_query_id is not None:\n",
    "                    # Skip sequences until the last processed query ID is found\n",
    "                    if seq_record.id != last_processed_query_id:\n",
    "                        continue\n",
    "                    # Reset last_processed_query_id after resuming\n",
    "                    last_processed_query_id = None\n",
    "                blastp_and_store_hits(seq_record, csv_writer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23792,
     "status": "ok",
     "timestamp": 1717674769983,
     "user": {
      "displayName": "Henry Taunt",
      "userId": "07657040803472197154"
     },
     "user_tz": -60
    },
    "id": "Hj2u5YvFYr37",
    "outputId": "baf0d852-90b3-4651-fcec-584b22ddd5b6"
   },
   "outputs": [],
   "source": [
    "# Module 9 - NCBI scrapper\n",
    "\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# List of URLs to search\n",
    "urls = [\n",
    "    \"https://www.ncbi.nlm.nih.gov/protein/1QRL\",\n",
    "    \"https://www.ncbi.nlm.nih.gov/protein/HKI82391.1\",\n",
    "    \"https://www.ncbi.nlm.nih.gov/protein/HIP29611.1\",\n",
    "    \"https://www.ncbi.nlm.nih.gov/protein/WP_018153337.1\",\n",
    "    \"https://www.ncbi.nlm.nih.gov/protein/MDK2849247.1\",\n",
    "    \"https://www.ncbi.nlm.nih.gov/protein/MDK2790423.1\",\n",
    "    \"https://www.ncbi.nlm.nih.gov/protein/WP_015898908.1\",\n",
    "    \"https://www.ncbi.nlm.nih.gov/protein/XP_006585949.1\",\n",
    "    \"https://www.ncbi.nlm.nih.gov/protein/XP_028212979.1\",\n",
    "    \"https://www.ncbi.nlm.nih.gov/protein/NP_001328574.1\",\n",
    "    \"https://www.ncbi.nlm.nih.gov/protein/2FOQ\",\n",
    "    \"https://www.ncbi.nlm.nih.gov/protein/AAA86945.1\",\n",
    "    \"https://www.ncbi.nlm.nih.gov/protein/AAA86944.1\",\n",
    "    \"https://www.ncbi.nlm.nih.gov/protein/BAF05590.1\",\n",
    "    \"https://www.ncbi.nlm.nih.gov/protein/WP_190245347.1\",\n",
    "    \"https://www.ncbi.nlm.nih.gov/protein/WP_003688976.1\",\n",
    "    \"https://www.ncbi.nlm.nih.gov/protein/AAZ83743.1\",\n",
    "    \"https://www.ncbi.nlm.nih.gov/protein/NP_990648.1\"\n",
    "]\n",
    "\n",
    "# Function to search for words in a webpage\n",
    "def search_words(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()  # Raise an exception for HTTP errors\n",
    "        html_content = response.text\n",
    "        # Refine the regular expression pattern to capture all occurrences of the words\n",
    "        matches = re.findall(r'\\b(?:alpha|beta|gamma)\\b|\\w*(?:alpha|beta|gamma)\\w*|\\b(?:alpha|beta|gamma)\\b(?=[^>]*<)', html_content, flags=re.IGNORECASE)\n",
    "        return matches\n",
    "    except Exception as e:\n",
    "        return str(e)\n",
    "\n",
    "# Loop through each URL and search for words\n",
    "for url in urls:\n",
    "    print(\"Searching\", url)\n",
    "    matches = search_words(url)\n",
    "    if isinstance(matches, list):\n",
    "        if matches:\n",
    "            print(\"Matches found:\", matches)\n",
    "        else:\n",
    "            print(\"No matches found\")\n",
    "    else:\n",
    "        print(\"Error occurred:\", matches)\n",
    "    print(\"=\"*50)  # Separator for readability\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNKEx2A+/zN8Z7bskc3OdQH",
   "mount_file_id": "1_tAmLqqlHPb3qUF5sEBb1yRH1QIGZblg",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
